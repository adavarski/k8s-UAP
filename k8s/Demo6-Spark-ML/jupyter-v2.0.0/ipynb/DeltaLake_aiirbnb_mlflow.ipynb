{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==1.8.0\n",
      "  Downloading mlflow-1.8.0-py3-none-any.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic\n",
      "  Downloading alembic-1.4.3-py2.py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting click>=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 463 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.14.0-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (2.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (1.19.4)\n",
      "Collecting docker>=4.0.0\n",
      "  Downloading docker-4.4.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse\n",
      "  Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 453 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gorilla\n",
      "  Downloading gorilla-0.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (0.3)\n",
      "Collecting simplejson\n",
      "  Downloading simplejson-3.17.2-cp38-cp38-manylinux2010_x86_64.whl (137 kB)\n",
      "\u001b[K     |████████████████████████████████| 137 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy<=1.3.13\n",
      "  Downloading SQLAlchemy-1.3.13.tar.gz (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 2.1 MB/s eta 0:00:01     |███████████████████▎            | 3.6 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (5.3.1)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.17.3 in /usr/lib/python3/dist-packages (from mlflow==1.8.0) (2.22.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from mlflow==1.8.0) (1.14.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (1.2.0)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.18.1.tar.gz (21 kB)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.14.1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 855 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Flask\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=3.0 in /usr/lib/python3/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow==1.8.0) (45.2.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->mlflow==1.8.0) (2020.5)\n",
      "Requirement already satisfied: prometheus_client in /usr/local/lib/python3.8/dist-packages (from prometheus-flask-exporter->mlflow==1.8.0) (0.9.0)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from Flask->mlflow==1.8.0) (2.11.2)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->mlflow==1.8.0) (1.1.1)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: sqlalchemy, prometheus-flask-exporter, databricks-cli\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.3.13-cp38-cp38-linux_x86_64.whl size=1226142 sha256=0b5c38ab7b7e84b080e0fe85a99d0ec90560e39b8ac129e42cc71e21c28ced23\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1b/5b/fe/df3abf130f4f66b437ea92eb98814d9d9fa80d70a6ee080b96\n",
      "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-py3-none-any.whl size=17160 sha256=d24c97c9588348722cec1b6be29beaa68747fd68498f2ce06bc6c95d83901748\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/12/1a/8d/0c016e06370d07f82def661b6cb7d91d4e6b4ff7f2982e9f2c\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.14.1-py3-none-any.whl size=100576 sha256=6e3b0cfd55547f8ccd44be126336f186a9f4811384c98e70fe633ad8f2ecb02f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f8/a7/7c/74d614edb0dea04c3bb450951ec35c8f62aa197302ad2c3baa\n",
      "Successfully built sqlalchemy prometheus-flask-exporter databricks-cli\n",
      "Installing collected packages: sqlalchemy, Mako, python-editor, alembic, querystring-parser, click, protobuf, websocket-client, docker, sqlparse, gorilla, simplejson, gunicorn, cloudpickle, smmap, gitdb, gitpython, itsdangerous, Werkzeug, Flask, prometheus-flask-exporter, tabulate, databricks-cli, mlflow\n",
      "\u001b[33m  WARNING: The script mako-render is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script alembic is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gunicorn is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script flask is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts databricks and dbfs are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mlflow is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Flask-1.1.2 Mako-1.1.3 Werkzeug-1.0.1 alembic-1.4.3 click-7.1.2 cloudpickle-1.6.0 databricks-cli-0.14.1 docker-4.4.1 gitdb-4.0.5 gitpython-3.1.11 gorilla-0.3.0 gunicorn-20.0.4 itsdangerous-1.1.0 mlflow-1.8.0 prometheus-flask-exporter-0.18.1 protobuf-3.14.0 python-editor-1.0.4 querystring-parser-1.2.4 simplejson-3.17.2 smmap-3.0.4 sqlalchemy-1.3.13 sqlparse-0.4.1 tabulate-0.8.7 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# api and object access\n",
    "os.environ['MLFLOW_TRACKING_URI'] = \"http://mlflow.data:5000\"\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://minio-service.data:9000\"\n",
    "# minio credentials\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"minio\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DeltaLake-airbnb\")\\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\",\"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\",\"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio-service.data.svc.cluster.local:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = spark.read.csv(\"s3a://airbnb/sf-airbnb.csv\", header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, listing_url: string, scrape_id: bigint, last_scraped: string, name: string, summary: string, space: string, description: string, experiences_offered: string, neighborhood_overview: string, notes: string, transit: string, access: string, interaction: string, house_rules: string, thumbnail_url: string, medium_url: string, picture_url: string, xl_picture_url: string, host_id: int, host_url: string, host_name: string, host_since: string, host_location: string, host_about: string, host_response_time: string, host_response_rate: string, host_acceptance_rate: string, host_is_superhost: string, host_thumbnail_url: string, host_picture_url: string, host_neighbourhood: string, host_listings_count: int, host_total_listings_count: int, host_verifications: string, host_has_profile_pic: string, host_identity_verified: string, street: string, neighbourhood: string, neighbourhood_cleansed: string, neighbourhood_group_cleansed: string, city: string, state: string, zipcode: string, market: string, smart_location: string, country_code: string, country: string, latitude: double, longitude: double, is_location_exact: string, property_type: string, room_type: string, accommodates: int, bathrooms: double, bedrooms: int, beds: int, bed_type: string, amenities: string, square_feet: int, price: string, weekly_price: string, monthly_price: string, security_deposit: string, cleaning_fee: string, guests_included: int, extra_people: string, minimum_nights: int, maximum_nights: int, minimum_minimum_nights: int, maximum_minimum_nights: int, minimum_maximum_nights: int, maximum_maximum_nights: int, minimum_nights_avg_ntm: double, maximum_nights_avg_ntm: double, calendar_updated: string, has_availability: string, availability_30: int, availability_60: int, availability_90: int, availability_365: int, calendar_last_scraped: string, number_of_reviews: int, number_of_reviews_ltm: int, first_review: string, last_review: string, review_scores_rating: int, review_scores_accuracy: int, review_scores_cleanliness: int, review_scores_checkin: int, review_scores_communication: int, review_scores_location: int, review_scores_value: int, requires_license: string, license: string, jurisdiction_names: string, instant_bookable: string, is_business_travel_ready: string, cancellation_policy: string, require_guest_profile_picture: string, require_guest_phone_verification: string, calculated_host_listings_count: int, calculated_host_listings_count_entire_homes: int, calculated_host_listings_count_private_rooms: int, calculated_host_listings_count_shared_rooms: int, reviews_per_month: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'summary',\n",
       " 'space',\n",
       " 'description',\n",
       " 'experiences_offered',\n",
       " 'neighborhood_overview',\n",
       " 'notes',\n",
       " 'transit',\n",
       " 'access',\n",
       " 'interaction',\n",
       " 'house_rules',\n",
       " 'thumbnail_url',\n",
       " 'medium_url',\n",
       " 'picture_url',\n",
       " 'xl_picture_url',\n",
       " 'host_id',\n",
       " 'host_url',\n",
       " 'host_name',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_is_superhost',\n",
       " 'host_thumbnail_url',\n",
       " 'host_picture_url',\n",
       " 'host_neighbourhood',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'host_has_profile_pic',\n",
       " 'host_identity_verified',\n",
       " 'street',\n",
       " 'neighbourhood',\n",
       " 'neighbourhood_cleansed',\n",
       " 'neighbourhood_group_cleansed',\n",
       " 'city',\n",
       " 'state',\n",
       " 'zipcode',\n",
       " 'market',\n",
       " 'smart_location',\n",
       " 'country_code',\n",
       " 'country',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'is_location_exact',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'amenities',\n",
       " 'square_feet',\n",
       " 'price',\n",
       " 'weekly_price',\n",
       " 'monthly_price',\n",
       " 'security_deposit',\n",
       " 'cleaning_fee',\n",
       " 'guests_included',\n",
       " 'extra_people',\n",
       " 'minimum_nights',\n",
       " 'maximum_nights',\n",
       " 'minimum_minimum_nights',\n",
       " 'maximum_minimum_nights',\n",
       " 'minimum_maximum_nights',\n",
       " 'maximum_maximum_nights',\n",
       " 'minimum_nights_avg_ntm',\n",
       " 'maximum_nights_avg_ntm',\n",
       " 'calendar_updated',\n",
       " 'has_availability',\n",
       " 'availability_30',\n",
       " 'availability_60',\n",
       " 'availability_90',\n",
       " 'availability_365',\n",
       " 'calendar_last_scraped',\n",
       " 'number_of_reviews',\n",
       " 'number_of_reviews_ltm',\n",
       " 'first_review',\n",
       " 'last_review',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'requires_license',\n",
       " 'license',\n",
       " 'jurisdiction_names',\n",
       " 'instant_bookable',\n",
       " 'is_business_travel_ready',\n",
       " 'cancellation_policy',\n",
       " 'require_guest_profile_picture',\n",
       " 'require_guest_phone_verification',\n",
       " 'calculated_host_listings_count',\n",
       " 'calculated_host_listings_count_entire_homes',\n",
       " 'calculated_host_listings_count_private_rooms',\n",
       " 'calculated_host_listings_count_shared_rooms',\n",
       " 'reviews_per_month']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(rawDF)\n",
    "\n",
    "rawDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: int, neighbourhood_cleansed: string, latitude: double, longitude: double, property_type: string, room_type: string, accommodates: int, bathrooms: double, bedrooms: int, beds: int, bed_type: string, minimum_nights: int, number_of_reviews: int, review_scores_rating: int, review_scores_accuracy: int, review_scores_cleanliness: int, review_scores_checkin: int, review_scores_communication: int, review_scores_location: int, review_scores_value: int, price: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columnsToKeep = [\n",
    "  \"host_is_superhost\",\n",
    "  \"cancellation_policy\",\n",
    "  \"instant_bookable\",\n",
    "  \"host_total_listings_count\",\n",
    "  \"neighbourhood_cleansed\",\n",
    "  \"latitude\",\n",
    "  \"longitude\",\n",
    "  \"property_type\",\n",
    "  \"room_type\",\n",
    "  \"accommodates\",\n",
    "  \"bathrooms\",\n",
    "  \"bedrooms\",\n",
    "  \"beds\",\n",
    "  \"bed_type\",\n",
    "  \"minimum_nights\",\n",
    "  \"number_of_reviews\",\n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\",\n",
    "  \"price\"]\n",
    "\n",
    "baseDF = rawDF.select(columnsToKeep)\n",
    "baseDF.cache().count()\n",
    "display(baseDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: int, neighbourhood_cleansed: string, latitude: double, longitude: double, property_type: string, room_type: string, accommodates: int, bathrooms: double, bedrooms: int, beds: int, bed_type: string, minimum_nights: int, number_of_reviews: int, review_scores_rating: int, review_scores_accuracy: int, review_scores_cleanliness: int, review_scores_checkin: int, review_scores_communication: int, review_scores_location: int, review_scores_value: int, price: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, translate\n",
    "\n",
    "fixedPriceDF = baseDF.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))\n",
    "\n",
    "display(fixedPriceDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: string, neighbourhood_cleansed: string, latitude: string, longitude: string, property_type: string, room_type: string, accommodates: string, bathrooms: string, bedrooms: string, beds: string, bed_type: string, minimum_nights: string, number_of_reviews: string, review_scores_rating: string, review_scores_accuracy: string, review_scores_cleanliness: string, review_scores_checkin: string, review_scores_communication: string, review_scores_location: string, review_scores_value: string, price: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fixedPriceDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "noNullsDF = fixedPriceDF.na.drop(subset=[\"host_is_superhost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns converted from Integer to Double:\n",
      " - host_total_listings_count\n",
      " - accommodates\n",
      " - bedrooms\n",
      " - beds\n",
      " - minimum_nights\n",
      " - number_of_reviews\n",
      " - review_scores_rating\n",
      " - review_scores_accuracy\n",
      " - review_scores_cleanliness\n",
      " - review_scores_checkin\n",
      " - review_scores_communication\n",
      " - review_scores_location\n",
      " - review_scores_value\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "integerColumns = [x.name for x in baseDF.schema.fields if x.dataType == IntegerType()]\n",
    "doublesDF = noNullsDF\n",
    "\n",
    "for c in integerColumns:\n",
    "  doublesDF = doublesDF.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "columns = \"\\n - \".join(integerColumns)\n",
    "print(f\"Columns converted from Integer to Double:\\n - {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: string, neighbourhood_cleansed: string, latitude: string, longitude: string, property_type: string, room_type: string, accommodates: string, bathrooms: string, bedrooms: string, beds: string, bed_type: string, minimum_nights: string, number_of_reviews: string, review_scores_rating: string, review_scores_accuracy: string, review_scores_cleanliness: string, review_scores_checkin: string, review_scores_communication: string, review_scores_location: string, review_scores_value: string, price: string, bedrooms_na: string, bathrooms_na: string, beds_na: string, review_scores_rating_na: string, review_scores_accuracy_na: string, review_scores_cleanliness_na: string, review_scores_checkin_na: string, review_scores_communication_na: string, review_scores_location_na: string, review_scores_value_na: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "imputeCols = [\n",
    "  \"bedrooms\",\n",
    "  \"bathrooms\",\n",
    "  \"beds\", \n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\"\n",
    "]\n",
    "\n",
    "for c in imputeCols:\n",
    "  doublesDF = doublesDF.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))\n",
    "\n",
    "display(doublesDF.describe())\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\", inputCols=imputeCols, outputCols=imputeCols)\n",
    "\n",
    "imputedDF = imputer.fit(doublesDF).transform(doublesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, price: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(imputedDF.select(\"price\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputedDF.filter(col(\"price\") == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "posPricesDF = imputedDF.filter(col(\"price\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, minimum_nights: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[minimum_nights: double, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(posPricesDF.select(\"minimum_nights\").describe())\n",
    "display(posPricesDF\n",
    "  .groupBy(\"minimum_nights\").count()\n",
    "  .orderBy(col(\"count\").desc(), col(\"minimum_nights\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[host_is_superhost: string, cancellation_policy: string, instant_bookable: string, host_total_listings_count: double, neighbourhood_cleansed: string, latitude: double, longitude: double, property_type: string, room_type: string, accommodates: double, bathrooms: double, bedrooms: double, beds: double, bed_type: string, minimum_nights: double, number_of_reviews: double, review_scores_rating: double, review_scores_accuracy: double, review_scores_cleanliness: double, review_scores_checkin: double, review_scores_communication: double, review_scores_location: double, review_scores_value: double, price: double, bedrooms_na: double, bathrooms_na: double, beds_na: double, review_scores_rating_na: double, review_scores_accuracy_na: double, review_scores_cleanliness_na: double, review_scores_checkin_na: double, review_scores_communication_na: double, review_scores_location_na: double, review_scores_value_na: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleanDF = posPricesDF.filter(col(\"minimum_nights\") <= 365)\n",
    "\n",
    "display(cleanDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDF.write.format(\"delta\").save(\"s3a://airbnb/airbnb-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE database airbnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use airbnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS airbnb using delta location 's3a://airbnb/airbnb-clean'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   airbnb|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|  airbnb|   airbnb|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
      "|host_is_superhost| cancellation_policy|instant_bookable|host_total_listings_count|neighbourhood_cleansed|latitude| longitude|property_type|      room_type|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|price|bedrooms_na|bathrooms_na|beds_na|review_scores_rating_na|review_scores_accuracy_na|review_scores_cleanliness_na|review_scores_checkin_na|review_scores_communication_na|review_scores_location_na|review_scores_value_na|\n",
      "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
      "|                t|            moderate|               t|                      1.0|      Western Addition|37.76931|-122.43386|    Apartment|Entire home/apt|         3.0|      1.0|     1.0| 2.0|Real Bed|           1.0|            180.0|                97.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|               10.0|170.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
      "|                f|strict_14_with_gr...|               f|                      2.0|        Bernal Heights|37.74511|-122.42102|    Apartment|Entire home/apt|         5.0|      1.0|     2.0| 3.0|Real Bed|          30.0|            111.0|                98.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|                9.0|235.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
      "|                f|strict_14_with_gr...|               f|                     10.0|        Haight Ashbury|37.76669| -122.4525|    Apartment|   Private room|         2.0|      4.0|     1.0| 1.0|Real Bed|          32.0|             17.0|                85.0|                   8.0|                      8.0|                  9.0|                        9.0|                   9.0|                8.0| 65.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
      "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from airbnb\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_rf(file_path, num_trees, max_depth):\n",
    "  with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "    # Create train/test split\n",
    "    spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "    airbnbDF = spark.read.parquet(\"s3a://airbnb/airbnb-clean/\")\n",
    "    (trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "    # Prepare the StringIndexer and VectorAssembler\n",
    "    categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "    indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "\n",
    "    numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "    assemblerInputs = indexOutputCols + numericCols\n",
    "    vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    # Log params: Num Trees and Max Depth\n",
    "    mlflow.log_param(\"num_trees\", num_trees)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "\n",
    "    rf = RandomForestRegressor(labelCol=\"price\",\n",
    "                               maxBins=40,\n",
    "                               maxDepth=max_depth,\n",
    "                               numTrees=num_trees,\n",
    "                               seed=42)\n",
    "\n",
    "    pipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])\n",
    "\n",
    "    # Log model\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # Log metrics: RMSE and R2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                            labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # Log artifact: Feature Importance Scores\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(),\n",
    "                                    rfModel.featureImportances)),\n",
    "                          columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "    # First write to local filesystem, then tell MLflow where to find that file\n",
    "    pandasDF.to_csv(\"/tmp/feature-importance.csv\", index=False)\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    mlflow.log_artifact(\"data\", artifact_path=\"airbnb.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/01/04 10:46:56 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  mlflow_rf(\"./data\",2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  mlflow_rf(\"./data\",3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
