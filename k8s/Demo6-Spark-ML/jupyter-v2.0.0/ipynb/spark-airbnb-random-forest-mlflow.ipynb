{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow==1.8.0 in /home/jovyan/.local/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (3.14.0)\n",
      "Requirement already satisfied: alembic in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (1.4.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (7.1.2)\n",
      "Requirement already satisfied: simplejson in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (3.17.2)\n",
      "Requirement already satisfied: Flask in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (1.1.2)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (0.14.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from mlflow==1.8.0) (1.14.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (1.1.5)\n",
      "Requirement already satisfied: sqlparse in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (0.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (1.19.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (5.3.1)\n",
      "Requirement already satisfied: querystring-parser in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.17.3 in /usr/lib/python3/dist-packages (from mlflow==1.8.0) (2.22.0)\n",
      "Requirement already satisfied: gorilla in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (0.3.0)\n",
      "Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (20.0.4)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (0.18.1)\n",
      "Requirement already satisfied: sqlalchemy<=1.3.13 in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (1.3.13)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (0.3)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from mlflow==1.8.0) (2.8.1)\n",
      "Requirement already satisfied: cloudpickle in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (1.6.0)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (3.1.11)\n",
      "Requirement already satisfied: docker>=4.0.0 in /home/jovyan/.local/lib/python3.8/site-packages (from mlflow==1.8.0) (4.4.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /home/jovyan/.local/lib/python3.8/site-packages (from alembic->mlflow==1.8.0) (1.0.4)\n",
      "Requirement already satisfied: Mako in /home/jovyan/.local/lib/python3.8/site-packages (from alembic->mlflow==1.8.0) (1.1.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/jovyan/.local/lib/python3.8/site-packages (from Flask->mlflow==1.8.0) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from Flask->mlflow==1.8.0) (2.11.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /home/jovyan/.local/lib/python3.8/site-packages (from Flask->mlflow==1.8.0) (1.0.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/jovyan/.local/lib/python3.8/site-packages (from databricks-cli>=0.8.7->mlflow==1.8.0) (0.8.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas->mlflow==1.8.0) (2020.4)\n",
      "Requirement already satisfied: setuptools>=3.0 in /usr/lib/python3/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow==1.8.0) (45.2.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from prometheus-flask-exporter->mlflow==1.8.0) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from gitpython>=2.1.0->mlflow==1.8.0) (4.0.5)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/jovyan/.local/lib/python3.8/site-packages (from docker>=4.0.0->mlflow==1.8.0) (0.57.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->mlflow==1.8.0) (1.1.1)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /home/jovyan/.local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.8.0) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# api and object access\n",
    "os.environ['MLFLOW_TRACKING_URI'] = \"http://mlflow.data:5000\"\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://minio-service.data:9000\"\n",
    "# minio credentials\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"minio\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:75: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"airbnb\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_rf(file_path, num_trees, max_depth):\n",
    "  with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "    # Create train/test split\n",
    "    spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "    airbnbDF = spark.read.parquet(\"./data/\")\n",
    "    (trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "    # Prepare the StringIndexer and VectorAssembler\n",
    "    categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "    indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "\n",
    "    numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "    assemblerInputs = indexOutputCols + numericCols\n",
    "    vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    # Log params: Num Trees and Max Depth\n",
    "    mlflow.log_param(\"num_trees\", num_trees)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "\n",
    "    rf = RandomForestRegressor(labelCol=\"price\",\n",
    "                               maxBins=40,\n",
    "                               maxDepth=max_depth,\n",
    "                               numTrees=num_trees,\n",
    "                               seed=42)\n",
    "\n",
    "    pipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])\n",
    "\n",
    "    # Log model\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # Log metrics: RMSE and R2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                            labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # Log artifact: Feature Importance Scores\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(),\n",
    "                                    rfModel.featureImportances)),\n",
    "                          columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "    # First write to local filesystem, then tell MLflow where to find that file\n",
    "    pandasDF.to_csv(\"/tmp/feature-importance.csv\", index=False)\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    mlflow.log_artifact(\"data\", artifact_path=\"airbnb.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/12/23 09:44:46 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  mlflow_rf(\"./data\",3,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  mlflow_rf(\"./data\",4,5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  mlflow_rf(\"./data\",3,5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_rf(file_path, num_trees, max_depth):\n",
    "  with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "    # Create train/test split\n",
    "    spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "    airbnbDF = spark.read.parquet(\"./data/\")\n",
    "    (trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "    # Prepare the StringIndexer and VectorAssembler\n",
    "    categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "    indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "\n",
    "    numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "    assemblerInputs = indexOutputCols + numericCols\n",
    "    vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    # Log params: Num Trees and Max Depth\n",
    "    mlflow.log_param(\"num_trees\", num_trees)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "\n",
    "    rf = RandomForestRegressor(labelCol=\"price\",\n",
    "                               maxBins=40,\n",
    "                               maxDepth=max_depth,\n",
    "                               numTrees=num_trees,\n",
    "                               seed=42)\n",
    "\n",
    "    pipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])\n",
    "\n",
    "    # Log model\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # Log metrics: RMSE and R2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                            labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # Log artifact: Feature Importance Scores\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(),\n",
    "                                    rfModel.featureImportances)),\n",
    "                          columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "    # First write to local filesystem, then tell MLflow where to find that file\n",
    "    pandasDF.to_csv(\"/tmp/feature-importance.csv\", index=False)\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    mlflow.log_artifact(\"data\", artifact_path=\"airbnb.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  mlflow_rf(\"./data\",3,5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
