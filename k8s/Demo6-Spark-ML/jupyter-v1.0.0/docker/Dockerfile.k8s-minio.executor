# Dependencies Container Image
# Install wget to retrieve Spark runtime components,
# extract to temporary directory, copy to the desired image
FROM ubuntu:18.04 AS deps

RUN apt-get update && apt-get -y install wget
WORKDIR /tmp
RUN wget https://apache.cs.utah.edu/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz \
	&& tar xvzf spark-3.0.1-bin-hadoop3.2.tgz 


# Runtime Container Image. Adapted from the official Spark runtime 
# image from the project repository at https://github.com/apache/spark.
FROM nvidia/cuda:11.0-runtime-ubuntu20.04 AS build

# Install Spark Dependencies and Prepare Spark Runtime Environment
RUN set -ex && \
    apt-get update && \
    ln -s /lib /lib64 && \
    mkdir -p /usr/share/man/man1 && \
    apt install -y bash tini libc6 libpam-modules libnss3 wget python3 python3-pip scala && \
    mkdir -p /opt/spark && \
    mkdir -p /opt/spark/work-dir && \
    touch /opt/spark/RELEASE && \
    rm /bin/sh && \
    ln -sv /bin/bash /bin/sh && \
    ln -sv /usr/bin/tini /sbin/tini && \
    echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su && \
    chgrp root /etc/passwd && chmod ug+rw /etc/passwd && \
    ln -sv /usr/bin/python3 /usr/bin/python && \
    ln -sv /usr/bin/pip3 /usr/bin/pip \
    rm -rf /var/cache/apt/*

# Configure timezone variables
ENV DEBIAN_FRONTEND=noninteractive

# Install R dependencies
RUN apt-get install -y --no-install-recommends fonts-dejavu unixodbc unixodbc-dev gfortran gcc \
    dirmngr ed locales ca-certificates software-properties-common 
RUN add-apt-repository --enable-source --yes "ppa:marutter/rrutter4.0" \
  && add-apt-repository --enable-source --yes "ppa:c2d4u.team/c2d4u4.0+"

# Configure default locale, see https://github.com/rocker-org/rocker/issues/19
RUN echo "en_US.UTF-8 UTF-8" >> /etc/locale.gen \
  && locale-gen en_US.utf8 \
  && /usr/sbin/update-locale LANG=en_US.UTF-8

ENV LC_ALL en_US.UTF-8
ENV LANG en_US.UTF-8
ENV TZ UTC

# Install R 
RUN apt-get update \
  && apt-get install -y --no-install-recommends littler r-base r-base-dev r-recommended \
  && ln -s /usr/lib/R/site-library/littler/examples/install.r /usr/local/bin/install.r \
  && ln -s /usr/lib/R/site-library/littler/examples/install2.r /usr/local/bin/install2.r \
  && ln -s /usr/lib/R/site-library/littler/examples/installGithub.r /usr/local/bin/installGithub.r \
  && ln -s /usr/lib/R/site-library/littler/examples/testInstalled.r /usr/local/bin/testInstalled.r \
  && install.r docopt \
  && rm -rf /tmp/downloaded_packages/ /tmp/*.rds

# Install Kerberos Client and Auth Components
RUN apt install -yqq krb5-user \
  && rm -rf /var/cache/apt/*

# Hadoop: Copy previously fetched runtime components
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/bin /opt/spark/bin
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/conf /opt/spark/conf
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/data /opt/spark/data
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/examples /opt/spark/examples
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/jars /opt/spark/jars
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/kubernetes /opt/spark/kubernetes
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/licenses /opt/spark/licenses
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/python /opt/spark/python
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/R /opt/spark/R
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/sbin /opt/spark/sbin
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/yarn /opt/spark/yarn

RUN curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.0.1/hadoop-aws-3.0.1.jar -o /opt/spark/jars/hadoop-aws-3.0.1.jar
RUN curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.11.923/aws-java-sdk-core-1.11.923.jar -o /opt/spark/jars/aws-java-sdk-core-1.11.923.jar
RUN curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.923/aws-java-sdk-1.11.923.jar -o /opt/spark/jars/java-sdk-1.11.923.jar
RUN curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-kms/1.11.923/aws-java-sdk-kms-1.11.923.jar -o /opt/spark/jars/aws-java-sdk-kms-1.11.923.jar
RUN curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.11.923/aws-java-sdk-s3-1.11.923.jar -o /opt/spark/jars/aws-java-sdk-s3-1.11.923.jar

# Copy Docker entry script
COPY --from=deps /tmp/spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh /opt/

# Set Spark environment
ENV SPARK_HOME /opt/spark
ENV PATH $PATH:$SPARK_HOME/bin

WORKDIR /opt/spark/work-dir
RUN chmod g+w /opt/spark/work-dir

ENTRYPOINT [ "/opt/entrypoint.sh" ]

# Specify the User that the actual main process will run as
USER ${spark_uid}
